{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwfD8xEN7fg2OR+6T50t4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyp2002/natural_processing_language/blob/main/Tokenizacion_Normalizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Class 1:**  **NLTK EN GOOGLE COLAB** üñ±"
      ],
      "metadata": {
        "id": "TpDGIDJpPxGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wym6mm0WPwXn",
        "outputId": "b84d6273-25e6-4ea0-a8d9-3e5bdb0f46a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#ntlk Tiene bastantes volumenes de texto para poder trabajar\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('cess_esp')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Expresiones Regulares: Constituyen un lenguaje estandarizado para definir patrones de busqueda de texto en cuerpos de texto\n",
        "#Librerioa de operaciones con expresiones regulares como re\n",
        "\n",
        "import re\n",
        "corpus = nltk.corpus.cess_esp.sents()\n",
        "print(corpus)\n",
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK--m0X5SFIx",
        "outputId": "de347819-e62c-418c-b9f6-5eda784d2f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricit√©_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunci√≥', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_√Åguila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japon√©s', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explic√≥', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcci√≥n', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prev√©', 'la', 'utilizaci√≥n', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Esto es un ciclo\n",
        "flatten = [w for l in corpus for w in l]\n",
        "print(flatten[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx6EaRjOSUha",
        "outputId": "d4561ac7-9754-46d0-cf0e-eaf93a4f913f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricit√©_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunci√≥', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ESTRUCTURA DE LA FUNCION re.search()**"
      ],
      "metadata": {
        "id": "CL0Wav9wbBjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Determina si el patron de busqueda p esta contenido en la cadena s\n",
        "#re.search('es',w)  es: expresion de la palabra -- w:Objeto w que esta pasando de flatten\n",
        "#Dado una estructura o patros este comando busca si se esta usando este patron de dise√±io devolviendo un true o false\n",
        "arr = [w for w in flatten if re.search('es',w)]\n",
        "\n",
        "#Meta carecter basico -- define un patron muy basico"
      ],
      "metadata": {
        "id": "-tY_fufLSFEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(arr[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5HyKZHSEy8",
        "outputId": "16205804-c543-4cca-8ba1-b4753676e9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cuando colocamos un $ despues de nuestro metacarecter buscamos en nuestra esprexion que este al final de las palabras\n",
        "arr = [w for w in flatten if re.search('es$',w)]\n",
        "print(arr[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzwkVRu_cdA1",
        "outputId": "5607ca82-439e-4f66-d95c-f9820e37decd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jueves', 'centrales', 'millones', 'millones', 'd√≥lares']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cuando colocamos un ^ antes de nuestro metacarecter buscamos en nuestra esprexion que este al inicio de las palabras\n",
        "arr = [w for w in flatten if re.search('^es',w)]\n",
        "print(arr[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FznMO2iec1ze",
        "outputId": "81908174-5a45-4d9a-953f-5e94f7f94c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['estatal', 'es', 'esta', 'esta', 'eso']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patrones de busquedas mas sofisticadas -- usando el concecpto de rango"
      ],
      "metadata": {
        "id": "X8lK1NukdHrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rango [a-z] [hgi]\n",
        "#Podemos definir la lista de caracteres que pueden estar en una posicion dado con las expresiones anteriores dentro de una cadena de texto\n",
        "arr = [w for w in flatten if re.search('^[ghi]',w)]\n",
        "print(arr[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEV7pmnwdG-2",
        "outputId": "9cd88c2b-d720-464d-8905-1b1c71c221b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['grupo', 'hoy', 'gas', 'gas', 'intervendr√°', 'invertir', 'gas', 'hoy', 'insulto', 'intervenci√≥n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clausuras ?\n",
        "# * EN una cadena de texto esta se puede repetir 0 o mas veces\n",
        "# + EN una cadena de texto esta se puede repetir 1 o mas veces\n",
        "arr = [w for w in flatten if re.search('^(no)*',w)]\n",
        "print(arr[:10])\n",
        "\n",
        "#En el ejemplo se nos muiestra que al usar el * no se nos estra mostranla la palabra no al inicio de nuestras cadenas de texto pero si poner +\n",
        "#este busca las cadenas de texto que tenga el no al inicio de la palabra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD4huncSdCID",
        "outputId": "ebd6fecd-605b-4634-f070-a8d5a3ad2d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricit√©_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunci√≥', 'hoy', ',']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizacion de Texto"
      ],
      "metadata": {
        "id": "g_Q5_UTFfeiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#raw\n",
        "#Poniendo un r al principio de texto nuestro python entendera que este es un texto plano\n",
        "print(r'esta es una \\n una pruba')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtgoGnU0fdHa",
        "outputId": "146d9bf0-798a-4e95-9164-2a80d890e9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "esta es una \\n una pruba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"\"\" Cuando sea el rey del mundo  (imaginaba √©l en su cabeza) no tendr√© que  preocuparme por estas bobadas.\n",
        "            Era solo un ni√±o de 7 a√±os, pero pensaba que podr√≠a ser cualquier cosa que su imaginaci√≥n le permitiera visualizar en su cabeza ...\"\"\"\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duymGcxjgO5u",
        "outputId": "5c6fd2c5-d2b2-456f-c531-416aef8ffcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cuando sea el rey del mundo  (imaginaba √©l en su cabeza) no tendr√© que  preocuparme por estas bobadas. \n",
            "            Era solo un ni√±o de 7 a√±os, pero pensaba que podr√≠a ser cualquier cosa que su imaginaci√≥n le permitiera visualizar en su cabeza ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos a toeknizar\n",
        "#1 Paso por espacios lo realziamos con re.split(r'',texot)  el r'' : nos dice que sera por espacio y el texto lo que queremos tokenizar\n",
        "print(re.split(r' ',texto))\n",
        "\n",
        "#Cuando tokenizamos deberiamos tener tokens mas importantes o que necesito por eso vamos a relaizar limpieza de texto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyPnQh3cgYR0",
        "outputId": "d3ba0c1b-2024-4c05-9d70-d701ac28f0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '', '(imaginaba', '√©l', 'en', 'su', 'cabeza)', 'no', 'tendr√©', 'que', '', 'preocuparme', 'por', 'estas', 'bobadas.', '\\n', '', '', '', '', '', '', '', '', '', '', '', 'Era', 'solo', 'un', 'ni√±o', 'de', '7', 'a√±os,', 'pero', 'pensaba', 'que', 'podr√≠a', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginaci√≥n', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Paso Tokenizar usando expresiones regulares\n",
        "print(re.split(r'[ \\t\\n]+',texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBrZ11idgYIj",
        "outputId": "a2c4b581-1591-4322-8bf5-d7afa5dd926c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '(imaginaba', '√©l', 'en', 'su', 'cabeza)', 'no', 'tendr√©', 'que', 'preocuparme', 'por', 'estas', 'bobadas.', 'Era', 'solo', 'un', 'ni√±o', 'de', '7', 'a√±os,', 'pero', 'pensaba', 'que', 'podr√≠a', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginaci√≥n', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Paso\n",
        "print(re.split(r'[ \\W\\t\\n]+',texto))\n",
        "\n",
        "#En si esta tokenizacion es muy basica y sencilla pero cuando haya que utilizar para textos mas conplejos tenemos la tokenizacion de NLTk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg5q-PzkgX4m",
        "outputId": "2133f4a4-d71b-4609-f8d5-fef82d194313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', 'imaginaba', '√©l', 'en', 'su', 'cabeza', 'no', 'tendr√©', 'que', 'preocuparme', 'por', 'estas', 'bobadas', 'Era', 'solo', 'un', 'ni√±o', 'de', '7', 'a√±os', 'pero', 'pensaba', 'que', 'podr√≠a', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginaci√≥n', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nuestra antigua regex no funciona en este caso:\n",
        "texto = 'En los E.U. esa postal vale $15.50 ...'\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))\n",
        "\n",
        "#Usando el caso 3 podemos ver varios errores como queriamos que sea un token el apartado de E.U o que los $15.50 sea otro token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JcTcW0HiKkz",
        "outputId": "372da5e6-f8d2-4608-e950-58f2b8c323e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'''(?xi)                   # Modo verbose e insensible a may√∫sculas y min√∫sculas\n",
        "              (?:[A-Z]\\.)+             # Coincide con abreviaciones como U.S.A.\n",
        "              | \\w+(?:-\\w+)*           # Coincide con palabras que pueden tener un gui√≥n interno\n",
        "              | \\$?\\d+(?:\\.\\d+)?%?     # Coincide con dinero o porcentajes como $15.5 o 100%\n",
        "              | \\.\\.\\.                 # Coincide con puntos suspensivos\n",
        "              | [.,;'\"?():_\\-]         # Coincide con signos de puntuaci√≥n espec√≠ficos\n",
        "              | \\w+(?:'\\w+)?           # Coincide con palabras que pueden contener ap√≥strofes\n",
        "'''\n",
        "\n",
        "nltk.regexp_tokenize(texto, pattern)\n",
        "\n",
        "#nltk.regexp_tokenize el (texto que queremos tokenizar),(otro es el patron de busqueda )\n",
        "#Es una de las mejores tokenizaciones que vimos ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v315w8TOiKeq",
        "outputId": "0bb6512c-eca4-41e1-dd48-2433e0c99eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'postal', 'vale', '$15.50', '...']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLOCACIONES"
      ],
      "metadata": {
        "id": "rlFDv69GkSN4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSS64ENCiKUz",
        "outputId": "f5a12b36-239e-4c7f-e047-def611c7fb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['En', 'los', 'E', '.', 'U', '.', 'esa', 'postal', 'vale', '15', '.', '50', '...']\n"
          ]
        }
      ]
    }
  ]
}